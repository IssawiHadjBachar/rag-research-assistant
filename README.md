# ğŸ§  RAG-Powered Research Paper Assistant

An intelligent **Retrieval-Augmented Generation (RAG)** assistant that helps you **analyze and query research papers**.
You can **upload PDFs** or **search arXiv**, and the system will build a semantic knowledge base using **SentenceTransformers** and **ChromaDB**, then answer your questions using **Google Gemini 1.5** through **LiteLLM**.

---

## ğŸš€ Features

- âœ… Upload and process multiple PDFs 
- âœ… Search and summarize academic papers from **arXiv** 
- âœ… Semantic text chunking and embedding via **SentenceTransformers** 
- âœ… Persistent vector database using **ChromaDB** 
- âœ… Context-aware question answering powered by **Gemini 1.5** 
- âœ… Easy-to-use web interface built with **Streamlit** 

---

## ğŸ—‚ï¸ Project Structure

```
ğŸ“¦ rag-research-assistant
 â”£ ğŸ“œ app.py                  # Main Streamlit application
 â”£ ğŸ“œ requirements.txt        # Python dependencies
 â”£ ğŸ“œ .env                    # API keys 
 â”— ğŸ“‚ chroma_db               # Local persistent ChromaDB folder
```

---

## ğŸ§° Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/IssawiHadjBachar/rag-research-assistant.git
cd rag-research-assistant
```

---

### 2ï¸âƒ£ Create and Activate a Virtual Environment

#### On macOS/Linux:

```bash
python3 -m venv venv
source venv/bin/activate
```

#### On Windows:

```bash
python -m venv venv
venv\Scripts\activate
```

---

### 3ï¸âƒ£ Install Dependencies

Make sure you have **Python 3.10+** installed, then run:

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Set Up Environment Variables

Create a `.env` file in the root directory with the following content:

```
GEMINI_API_KEY="" 
HUGGINGFACE_TOKEN=""
```

**Where to get the keys:**

* [Google AI Studio](https://aistudio.google.com/api-keys)
* [Hugging Face](https://huggingface.co/settings/tokens)

---
## âš™ï¸ How It Works
1- PDF/ArXiv Input â†’ Extracts or fetches text.

2- Text Chunking â†’ Uses RecursiveCharacterTextSplitter to create semantically meaningful chunks.

3- Embedding Generation â†’ Each chunk is converted into a vector using all-MiniLM-L6-v2 (SentenceTransformers).

4- Vector Storage â†’ Embeddings are saved into a persistent ChromaDB collection (knowledge_base).

5- Query â†’ User asks a question. The query is embedded and matched to the most relevant text chunks.

6- Response Generation â†’ The top results are passed as context to Gemini 1.5 to generate a structured and relevant answer.

---
## ğŸ§ª Usage

Run the app with Streamlit:

```bash
streamlit run main.py
```


---
